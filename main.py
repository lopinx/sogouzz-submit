# -*- coding: utf-8 -*-
__author__ = "https://github.com/lopinx"
__version__ = "1.1.0"
# =================================================================================================
# æ·»åŠ è°ƒè¯•æ¨¡å¼å¯åŠ¨ï¼š 
# "Google Chrome": å¿…é¡»å…ˆç»“æŸå·²å­˜åœ¨çš„Chromeæµè§ˆå™¨è¿›ç¨‹åå†å¼€å¯ï¼Œ 
# Windowsï¼šchrome.exe --headless=new --remote-debugging-port=39222
# Linux/MacOS:  google-chrome --headless=new --remote-debugging-port=39222
# google-chrome --headless=new --remote-debugging-port=0 # éšæœºåˆ†é…ä¸€ä¸ªå¯ç”¨ç«¯å£
#  å¯åŠ¨ -> è®¿é—®"http://localhost:39222/json/version" -> è·å–ä»¥ä¸‹JSONæ•°æ®ä¸­çš„"webSocketDebuggerUrl"
"""
``` json
{
    "Browser": "Chrome/135.0.7049.85",
    "Protocol-Version": "1.3",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36",
    "V8-Version": "13.5.212.10",
    "WebKit-Version": "537.36 (@1e112499da812a1dde62101ed601dcb93024aaff)",
    "webSocketDebuggerUrl": "ws://localhost:39222/devtools/browser/6842b2f4-1219-44d9-836d-fc5fdf65a74b"
}
```
"""
# "Microsoft Edge": å¿…é¡»å…ˆç»“æŸå·²å­˜åœ¨çš„Edgeæµè§ˆå™¨è¿›ç¨‹åå†å¼€å¯
# Windowsï¼šmsedge.exe --headless=new --remote-debugging-port=39333
# Linux/MacOS:  microsoft-edge --headless=new --remote-debugging-port=39333
# microsoft-edge --headless=new --remote-debugging-port=0 # éšæœºåˆ†é…ä¸€ä¸ªå¯ç”¨ç«¯å£
#  å¯åŠ¨ -> è®¿é—®"http://localhost:39333/json/version" -> è·å–ä»¥ä¸‹JSONæ•°æ®ä¸­çš„"webSocketDebuggerUrl"
# https://learn.microsoft.com/zh-cn/microsoft-edge/devtools-protocol-chromium/
"""
``` json
{
    "Browser": "Chrome/135.0.7049.85",
    "Protocol-Version": "1.3",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36",
    "V8-Version": "13.5.212.10",
    "WebKit-Version": "537.36 (@1e112499da812a1dde62101ed601dcb93024aaff)",
    "webSocketDebuggerUrl": "ws://localhost:39333/devtools/browser/6842b2f4-1219-44d9-836d-fc5fdf65a74b"
}
```
"""
# =================================================================================================
# (é¡¹ç›®æ–°å»º)å®‰è£…ä¾èµ–ï¼šuv add ddddocr pyjson5 psutil websockets playwright
# (é¡¹ç›®é‡å»º)åŒæ­¥ä¾èµ–ï¼šuv sync 
# (å¯¼å‡ºä¾èµ–)ï¼šuv pip freeze | uv pip compile - -o requirements.txt
# =================================================================================================
# playwright æµè§ˆå™¨è‡ªåŠ¨åŒ–æ¡†æ¶
# å®‰è£…æµè§ˆå™¨é©±åŠ¨Chromium, Firefox, WebKitï¼Œä¸æ·»åŠ å‚æ•°åˆ™ä¸ºå…¨éƒ¨
# uv run python -m playwright install chromium
# uv run playwright codegen  #å¼€å¯å½•åˆ¶
# ä½¿ç”¨ launchï¼šå½“éœ€è¦ç›´æ¥æ§åˆ¶æµè§ˆå™¨ç”Ÿå‘½å‘¨æœŸï¼ˆå¦‚è‡ªåŠ¨åŒ–æµ‹è¯•ï¼‰æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ launchã€‚
# playwright.chromium.launch(headless=False) # ç›´æ¥å¯åŠ¨æ–°æµè§ˆå™¨å®ä¾‹
# ä½¿ç”¨ CDP è¿æ¥ï¼šå½“éœ€è¦è¿æ¥å·²æœ‰æµè§ˆå™¨å®ä¾‹ï¼ˆå¦‚å¤ç”¨ç™»å½•çŠ¶æ€ã€è¿œç¨‹è°ƒè¯•ï¼‰æ—¶ï¼Œé€šè¿‡ connect_over_cdp è¿æ¥ã€‚
# playwright.chromium.connect_over_cdp("ws://localhost:9222") # è¿æ¥å·²è¿è¡Œæµè§ˆå™¨å®ä¾‹
# browser = p.chromium.launch(channel="chrome"ï¼Œheadless=False)
# browser = p.chromium.launch(channel="msedge"ï¼Œheadless=False)
# browser = p.firefox.launch(headless=False)
# browser = p.webkit.launch(headless=False)
# =================================================================================================
# è¿è¡Œå‰æ­¥éª¤ï¼š
# 1. å®‰è£…å¹¶å¯åŠ¨ Lightpanda æœåŠ¡(CentOS)æˆ–è€…æœ¬åœ°å¼€å¯Debuggeræ¨¡å¼ï¼ˆWin+Rï¼‰ï¼š 
# msedge.exe --headless=new --remote-debugging-port=39333
"""
``` bash
curl -L -o lightpanda https://github.com/lightpanda-io/browser/releases/download/nightly/lightpanda-x86_64-linux \
&& sudo mv lightpanda /usr/local/bin/ \
&& sudo chmod a+x /usr/local/bin/lightpanda \
&& echo 'export LIGHTPANDA_DISABLE_TELEMETRY=true' | sudo tee -a /etc/profile && sudo sh -c 'source /etc/profile' \
&& nohup lightpanda serve --host 0.0.0.0 --port 9222 > /dev/null 2>&1 & exit
```
"""
# 2. å®‰è£…ä¾èµ–ï¼šuv sync
# 3. è¿è¡Œç¨‹åºï¼šuv run python main.py
# =================================================================================================
import argparse
import asyncio
import logging
import random
import re
import subprocess
import sys
import tempfile
import time
from pathlib import Path
# import threading
from typing import Dict, List
from urllib.error import HTTPError, URLError
from urllib.parse import urljoin, urlparse
from urllib.request import Request, urlopen

import ddddocr
import psutil
import pyjson5 as json
import websockets
from playwright.async_api import (Browser, BrowserContext, Page, async_playwright)

# from playwright._impl._errors import Error,TimeoutError
# from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed

# =================================================================================================
system = sys.platform
OCR = ddddocr.DdddOcr(show_ad=False, beta=True)
OCR.set_ranges("0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ") # è¯†åˆ«èŒƒå›´
# =================================================================================================
# ç¡®å®šå·¥ä½œç›®å½•ï¼ˆå¤„ç†æ‰“åŒ…åçš„è·¯å¾„ï¼‰
WorkDIR = Path(__file__).resolve().parent
# æ—¥å¿—è®°å½•é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - L%(lineno)d - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logging.info(f"ğŸš€ å¯åŠ¨ç¨‹åº {__author__}")
# è¯»å–é…ç½®æ–‡ä»¶dev.json,config.jsonåˆ†åˆ«ä¸ºå¼€å‘å’Œç”Ÿäº§ç¯å¢ƒ
parser = argparse.ArgumentParser(description="é…ç½®æ–‡ä»¶å‚æ•°")
parser.add_argument(
    "conf",
    type=str,
    help="æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ï¼šconfig.jsonï¼Œé»˜è®¤ä½¿ç”¨å½“å‰ç›®å½•çš„ config.jsonï¼‰",
    nargs='?',
    default='dev.json' if (WorkDIR / 'dev.json').exists() else 'config.json'
)
args = parser.parse_args()
try:
    with (conf_path := (WorkDIR / args.conf)).open('r', encoding='utf-8') as f:
        config = json.load(f)
    logging.info(f"ğŸ¥° é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸï¼")
except FileNotFoundError:
    logging.error(f"é”™è¯¯ï¼šé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼")
    raise
except json.Json5Error:
    logging.error(f"é”™è¯¯ï¼šé…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼")
    raise
# =================================================================================================

async def wslink() -> List[str]:
    """è·å–æœ¬åœ°è°ƒè¯•é“¾æ¥"""
    logging.info(f"ğŸ¥° å¼€å§‹éå†å¯ç”¨çš„æµè§ˆå™¨æ¥å£é“¾æ¥ï¼")
    ws_url = []
    if cdp := config.get("cdpserver"):
        for url in cdp:
            try:
                async with websockets.connect(url, open_timeout=5) as ws:
                    await ws.ping()
                    ws_url.append(url)
            except:
                continue
    if len(ws_url) == 0:
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            if name := proc.info['name'].lower():
                # è¿›ç¨‹åç§°è¿‡æ»¤ï¼ˆChrome/Edge/Chromiumï¼‰
                if not re.match(r'^(chrome|edge|msedge|microsoft-edge|chromium)(\.exe)?$', name, re.IGNORECASE) and len(name) > 1:
                    continue
            try:
                cmdline = ' '.join(proc.info['cmdline'] or []).lower()
                if '--remote-debugging-port=' not in cmdline:
                    continue
                port_match = re.search(r'--remote-debugging-port=(\d+)', cmdline)
                if not port_match:
                    continue
                port = int(port_match.group(1))
                if port == 0:
                    for conn in proc.net_connections():
                        if conn.status == 'LISTEN' and 1024 <= (port := conn.laddr.port) <= 65535:
                            break
                try:
                    req = Request(f"http://localhost:{port}/json/version", headers={'User-Agent': 'Mozilla/5.0'})
                    with urlopen(req, timeout=1) as r:
                        if ws := json.loads(r.read().decode()).get('webSocketDebuggerUrl'):
                            ws_url.append(ws)
                except:
                    continue
            except:
                continue
        if len(ws_url) == 0:
            browsers = [
                {
                    "name": "chrome",
                    "path": {
                        "win32": r"C:\Program Files\Google\Chrome\Application\chrome.exe",
                        "linux": "/usr/bin/google-chrome",
                    },
                    "port": 39222
                },
                {
                    "name": "edge",
                    "path": {
                        "win32": r"C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe",
                        "linux": "/usr/bin/microsoft-edge",
                    },
                    "port": 39333
                }
            ]
            for browser in browsers:
                path_str = browser["path"].get(system)
                if not path_str:
                    logging.warning(f"âš ï¸ {system}æœªé…ç½®{browser['name']}è·¯å¾„")
                    continue
                path = Path(path_str)
                if not path.exists():
                    logging.warning(f"âš ï¸ {path}æœªæ‰¾åˆ°{browser['name']}")
                    continue
                if any(proc.info.get('name') == path.name for proc in psutil.process_iter(['name'])):
                    logging.warning(f"âš ï¸  {browser['name']}æ­£åœ¨è¿è¡Œï¼Œè·³è¿‡")
                    continue
                try:
                    # å¯åŠ¨å‚æ•°ï¼šç¦ç”¨QQBrowserå¯¼å…¥ + å…³é—­æ—¥å¿—ï¼ˆWeChatå½±å“ï¼‰
                    (edge_data := Path(tempfile.mkdtemp(prefix="edge_temp_"))).mkdir(parents=True, exist_ok=True)
                    args = [
                        str(path),
                        "--headless=new",
                        f"--remote-debugging-port={browser['port']}",
                        "--disable-features=ImportQQBrowserData",   # ç¦ç”¨QQBrowseræ•°æ®å¯¼å…¥
                        "--disable-logging",                        # å…³é—­æ‰€æœ‰æ—¥å¿—
                        "--noerrdialogs",                           # éšè—é”™è¯¯å¯¹è¯æ¡†
                        f"--user-data-dir={edge_data}"              # ä½¿ç”¨ä¸´æ—¶ç›®å½•
                    ]
                    # å¯åŠ¨ Edge å¹¶ä¸¢å¼ƒæ‰€æœ‰é”™è¯¯è¾“å‡º
                    subprocess.Popen(args) # , stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
                    time.sleep(3)
                    req = Request(f"http://localhost:{browser['port']}/json/version", method='GET')
                    with urlopen(req, timeout=1) as r:
                        data = json.loads(r.read().decode())
                        if ws := data.get('webSocketDebuggerUrl'):
                            ws_url.append(ws)
                            print(f"ğŸš€ {browser['name']}æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼Œç«¯å£ï¼š{browser['port']}")
                            break
                except Exception as e:
                    print(f"âŒ å¯åŠ¨{browser['name']}å¤±è´¥: {str(e)}")
    return list(set(ws_url))

async def captcha(page: Page, captcha_selector: str, input_selector: str, OCR: any) -> None:
    """å¤„ç†éªŒè¯ç çš„é€šç”¨å‡½æ•°"""
    if await page.locator(captcha_selector).is_visible():
        await page.wait_for_timeout(1000)
        captcha_obj = page.locator(captcha_selector)
        captcha_code = OCR.classification(await captcha_obj.screenshot(type='png'))
        await page.fill(input_selector, captcha_code)
        await page.wait_for_timeout(1000)
    else:
        logging.warning("âŒ éªŒè¯ç å…ƒç´ æœªæ‰¾åˆ°ï¼Œé¡µé¢å¯èƒ½æœªå®Œå…¨åŠ è½½")

async def login(page: Page, site: Dict, OCR: any) -> bool:
    """å¤„ç†ç™»å½•é€»è¾‘"""
    _login = False
    if await page.locator('//a[@class="logout"]').is_visible(timeout=180000):
        _login = True
        logging.info(f"âœ… ã€æ— éœ€ç™»å½•ã€‘ï¼Œå› ä¸ºå·²ç»ç™»å½•è¿‡äº†ï¼")
    else:
        await page.wait_for_timeout(1000)
        await page.click('//a[@class="login"]')
        await page.wait_for_timeout(1000)
        await page.fill('//input[@type="text" and @placeholder="å¹³å°è´¦å·"]', site['username'])
        await page.fill('//input[@type="password" and @placeholder="ç™»é™†å¯†ç "]', site['password'])
        await page.wait_for_timeout(1000)
        for attempt in range(config.get('captcha', 3)):
            try:
                await captcha(
                    page, 
                    '//*[@class="code_img"]/img',
                    '//input[@type="text" and @placeholder="éªŒè¯ç "]',
                    OCR
                )
            except Exception as e:
                logging.error(f"âŒ éªŒè¯ç å¤„ç†å¤±è´¥ï¼š{str(e)}")
                continue
            await page.wait_for_timeout(1000)
            await page.click('//a[@class="btn-login"]')
            await page.wait_for_timeout(1000)
            if await page.locator('//a[@class="logout"]').is_visible(timeout=180000):
                _login = True
                logging.info(f"âœ… ã€ç¬¬ {attempt+1} æ¬¡ã€‘ç™»å½•æˆåŠŸï¼")
                break
            else:
                await page.click('//a[@class="code_img"]')
                await page.wait_for_timeout(1000)
                continue
    return _login

async def urls(sitemap: str) -> List[str]:
    """ä»sitemapä¸­è·å–URLåˆ—è¡¨"""
    # logging.info(f"ğŸ’ª æ­£åœ¨å¥‹åŠ›çˆ¬å–ç½‘ç«™åœ°å›¾å“¦ï¼")
    _links, urls = "", []
    for attempt in range(config.get('captcha', 3)):
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36'}
            request = Request(sitemap, headers=headers)
            with urlopen(request) as resp:
                _links = resp.read().decode('utf-8')
            break
        except (HTTPError, URLError) as e:
            logging.error(f"âŒ æ— æ³•è·å–Sitemap: {str(e)}")
            continue

    if '<?xml' in _links[:100]:
        if '<urlset' in _links or '<sitemapindex' in _links:
            if '<sitemapindex' in _links:
                mapurls = re.findall(r'<loc>\s*(.*?)\s*</loc>', _links)
                urls = []
                for su in mapurls:
                    logging.info(f"ğŸ’ª å¼€å§‹çˆ¬å–ç½‘ç«™åœ°å›¾ï¼š{su}")
                    try:
                        with urlopen(su) as resp:
                            _sublinks = resp.read().decode('utf-8')
                        urls.extend(re.findall(r'<loc>\s*(.*?)\s*</loc>', _sublinks))
                    except URLError:
                        continue
            else:
                logging.info(f"ğŸ’ª å¼€å§‹çˆ¬å–ç½‘ç«™åœ°å›¾ï¼š{sitemap}")
                urls = re.findall(r'<loc>\s*(.*?)\s*</loc>', _links)
        elif '<rss' in _links or '<feed' in _links:
            logging.info(f"ğŸ’ª å¼€å§‹çˆ¬å–ç½‘ç«™åœ°å›¾ï¼š{sitemap}")
            urls = re.findall(r'<link>\s*(https?://.*?)\s*</link>', _links)
            if not urls:
                urls.extend(re.findall(r'<link[^>]+href="(https?://[^"]+)"', _links))
                urls.extend(re.findall(r'<link[^>]+rel="alternate"[^>]+href="(https?://[^"]+)"', _links))
    else:
        if '<!DOCTYPE html' in _links[:100] or '<html' in _links[:100]:
            logging.info(f"ğŸ’ª å¼€å§‹çˆ¬å–ç½‘ç«™åœ°å›¾ï¼š{sitemap}")
            _baseUrl = urlparse(sitemap).scheme + '://' + urlparse(sitemap).netloc
            urls = re.findall(r'<a\s+href=["\'](https?://[^"\']+)["\']', _links, re.IGNORECASE)
            # è¡¥å…¨ç›¸å¯¹é“¾æ¥å¹¶ä¸”è¿‡æ»¤éæœ¬åŸŸåçš„é“¾æ¥
            urls = [urljoin(_baseUrl, url) if not urlparse(url).netloc else url for url in urls]
            urls = [url for url in urls if urlparse(url).netloc == urlparse(_baseUrl).netloc]
        else:
            logging.info(f"ğŸ’ª å¼€å§‹çˆ¬å–ç½‘ç«™åœ°å›¾ï¼š{sitemap}")
            urls = [url.strip() for url in _links.splitlines() if url.strip()]
    # å‰”é™¤é¦–é¡µé“¾æ¥
    urls = list(dict.fromkeys(url for url in urls if not url.endswith('/') and not urlparse(url).path == ''))
    return urls

async def submit(page: Page, _batch: List[str], site: Dict, OCR: any, _index: int) -> bool:
    """æäº¤URLæ‰¹æ¬¡"""
    _submit, _box = False, False
    domain = urlparse(site['sitemap']).netloc
    for attempt in range(config.get('captcha', 3)):
        try:
            await page.click('//div[contains(@class,"website_box")]')
            _box = True
            break
        except Exception as e:
            if 'Page.click: Timeout' in str(e):
                await page.evaluate("""() => {
                    const hideElements = (selectors) => {
                        selectors.forEach(selector => {
                            const el = document.querySelector(selector);
                            if (el) el.style.display = 'none';
                        });
                    };
                    hideElements(['.mask_bg', '.pop_del_area']);
                }""")
                element = page.locator("//div[contains(@class,'website_box')]")
                await element.wait_for(state="visible", timeout=10000)
                await element.evaluate("elem => elem.click()")
                _box = True
                break
            else:
                logging.error(f"âŒ ç‚¹å‡»å…ƒç´ å¤±è´¥ï¼š{str(e)}")
                await page.wait_for_timeout(1000)
                continue
    if not _box:
        logging.error(f"âŒ æ— æ³•æ‰¾åˆ°å…ƒç´ ï¼Œå¯èƒ½æ˜¯é¡µé¢åŠ è½½ä¸å®Œå…¨æˆ–å…ƒç´ ä¸å­˜åœ¨ã€‚")
        return False
    try:
        await page.wait_for_timeout(1000)
        await page.fill('//input[contains(@type,"text") and @class="search_input"]', domain)
        await page.wait_for_timeout(1000)
        await page.click(f"//li[contains(@class, 'select_item') and normalize-space()='{domain}']")
    except Exception as e:
        logging.error(f"âŒ æ²¡æœ‰æ‰¾åˆ°æ‚¨çš„åŸŸåï¼Œè¯·ç¡®è®¤æ‚¨æ‹¥æœ‰è¯¥ç«™ç‚¹æƒé™!!!")
        return False
    await page.wait_for_timeout(1000)
    await page.fill('//div[@class="form-control"]//textarea', '\n'.join(_batch))
    await page.wait_for_timeout(1000)
    for attempt in range(config.get('captcha', 3)):
        try:
            await captcha(
                page,
                '//*[@class="form-control verification"]//img',
                '//*[@class="form-control verification"]//input[@type="text"]',
                OCR
            )
        except Exception as e:
            logging.error(f"âŒ éªŒè¯ç å¤„ç†å¤±è´¥ï¼š{str(e)}")
            continue
        await page.wait_for_timeout(1000)
        await page.click('//a[@class="btn_add"]')
        await page.wait_for_timeout(1000)
        if await page.locator('//a[@class="btn_pop"]').is_visible(timeout=180000):
            await page.click('//a[@class="btn_pop"]')
            await page.wait_for_timeout(1000)
            _submit = True
            logging.info(f"âœ… ã€ç¬¬ {_index + 1} æ‰¹ - ç¬¬ {attempt+1} æ¬¡ã€‘ï¼Œæ¨é€ {len(_batch)} æ¡URL æˆåŠŸï¼")
            break
        else:
            await page.click('//*[@class="form-control verification"]//img')
            await page.wait_for_timeout(1000)
            continue
    return _submit

async def main(site: Dict) -> bool:
    # è¯»å–æ—¥å¿—å¹¶è¿‡æ»¤å·²å¤„ç†çš„URL
    try:
        urls_list = await urls(site['sitemap'])
        if not urls_list:
            logging.error("âŒ æœªèƒ½ä»æ‚¨æä¾›çš„Sitemapåœ°å€è·å–åˆ°URL")
            return False
        
        _cps = []
        try:
            with open(WorkDIR / f'{urlparse(site['sitemap']).netloc}.log', 'r') as f:
                lines = f.read().splitlines()
                _cps = [int(line.strip()) for line in lines if line.strip().isdigit()]
        except FileNotFoundError:
            logging.warning("âš ï¸  æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†é‡æ–°å¤„ç†æ‰€æœ‰URL")

        urls_list = [url for idx, url in enumerate(urls_list) if idx not in _cps]
        if not urls_list:
            logging.error("âŒ æ‰€æœ‰URLå‡å·²å¤„ç†å®Œæ¯•ä¸”æ²¡æœ‰æ–°çš„URLåœ°å€")
            return False
    except URLError as e:
        logging.error(f"âŒ ç½‘ç»œé”™è¯¯ï¼Œæ— æ³•è·å–: {str(e)}")
        return False

    try:
        async with async_playwright() as playwright:
            # æµè§ˆå™¨çš„å‚æ•°
            _cargs = {
                "viewport": {"width": 1440, "height": 900},     # è®¾ç½®æµè§ˆå™¨çª—å£å¤§å°
                "screen": {"width": 1920, "height": 1080},      # è®¾ç½®å±å¹•åˆ†è¾¨ç‡
                "ignore_https_errors": True,                    # å¿½ç•¥httpsé”™è¯¯
                "java_script_enabled": True,                    # å¯ç”¨JavaScript
                "locale": "zh-CN",                              # è®¾ç½®è¯­è¨€ç¯å¢ƒ
                "timezone_id": "Asia/Shanghai",                 # è®¾ç½®æ—¶åŒº
                # "geolocation": {"longitude": 116.40387, "latitude": 39.91435, "accuracy": 100},   # è®¾ç½®åœ°ç†ä½ç½®
                "permissions": ["notifications"],               # è®¾ç½®æƒé™: é€šçŸ¥
                "bypass_csp": True,                             # ç»•è¿‡ CSPï¼ˆContent Security Policyï¼‰
                "accept_downloads": True,                       # å…è®¸ä¸‹è½½
                "user_agent": 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36'
            }

            # ç­›é€‰å¯ç”¨WSé“¾æ¥
            ws_link = await wslink()
            print(f"æ­£åœ¨ä½¿ç”¨CDPæœåŠ¡ï¼š{ws_link}")
            if ws_link:
                _ws = random.choice(ws_link)
            else:
                return False

            # è¿æ¥åˆ°æµè§ˆå™¨        
            try:
                browser: Browser = await playwright.chromium.connect_over_cdp(
                    _ws, 
                    timeout=180000,
                    slow_mo=500
                )
            except Exception as e:
                logging.error(f"âŒ æ— æ³•è¿æ¥åˆ°CDPæœåŠ¡: {str(e)}")
                return False
            
            if browser.contexts:
                context: BrowserContext = browser.contexts[0]
            else:
                context = await browser.new_context()
            page: Page = await context.new_page()

            # print(f"""æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...ï¼ˆ{config.get('backend')}ï¼‰""")
            # æµ‹è¯•ååçˆ¬è™«
            # await page.goto('https://res.cdn.issem.cn', wait_until="networkidle", timeout=18000000)
            
            # ç™»å½•ç«™é•¿å¹³å°
            await page.goto(config.get("backend"), wait_until='domcontentloaded', timeout=180000)
            await page.wait_for_timeout(1000)
            if not await login(page, site, OCR):
                return False

            await page.goto(f"{config.get('backend')}index.php/sitelink/index", wait_until='domcontentloaded', timeout=180000)
            
            # åˆ†æ‰¹å¤„ç†é“¾æ¥
            # _sizeä¸ºæ¯æ‰¹æ¬¡æ•°é‡ï¼Œbatchä¸ºå·²å®Œæˆæ‰¹æ¬¡ï¼Œbatchesä¸ºæ€»æ‰¹æ¬¡
            # _bacthä¸ºå½“å‰æ‰¹æ¬¡ï¼Œ_bTagä¸ºå½“å‰æ‰¹æ¬¡æ˜¯å¦æˆåŠŸ
            _size = 20
            batch, batches = set(), (len(urls_list) + _size - 1) // _size
            while len(batch) < batches:
                for _index, _start in enumerate(range(0, len(urls_list), _size)):
                    if _index in batch:
                        continue
                    _batch, _bTag = urls_list[_start:_start + _size], False     
                    for _ in range(config.get('captcha', 3)):
                        try:
                            if await submit(page, _batch, site, OCR, _index):
                                _bTag = True
                                batch.add(_index)
                                with open(WorkDIR / f'{urlparse(site['sitemap']).netloc}.log', 'a+') as f:
                                    for url in _batch:
                                        f.write(f"{url}\n")
                                break
                            await page.wait_for_timeout(1000)
                        except Exception as e:
                            await page.wait_for_timeout(1000)
                            continue 
                    if not _bTag:
                        logging.warning(f"âš ï¸  ç¬¬ {_index + 1} æ‰¹æ‰€æœ‰å°è¯•å‡å¤±è´¥ï¼Œå°†åœ¨ä¸‹æ¬¡å¾ªç¯é‡è¯•")
                if len(batch) < batches:
                    logging.warning(f"â³ æ‰¹æ¬¡å¤„ç†æœªå®Œæˆï¼š{len(batch)}/{batches}ï¼Œå‡†å¤‡å¼€å§‹æ–°ä¸€è½®é‡è¯•...")
            return True
    except Exception as e:
        logging.error(f"âŒ åˆ›å»ºé¡µé¢å¤±è´¥ï¼š{str(e)}")
        return False
    finally:
        # æ¸…ç†èµ„æºï¼ˆå…³é—­é¡µé¢å’Œä¸Šä¸‹æ–‡ï¼Œä¿ç•™æµè§ˆå™¨ï¼‰
        try:
            # å…³é—­å½“å‰é¡µé¢ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if page in locals()  and not page.is_closed():
                await page.close()
            # å…³é—­å½“å‰ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if context in locals() and not context.is_closed():
                await context.close()
            # ä¸å…³é—­æµè§ˆå™¨
            # await browser.close()  # æ³¨é‡Šæ‰æ­¤è¡Œ
        except Exception as e:
            logging.error(f"âš ï¸  æ¸…ç†èµ„æºæ—¶å‡ºé”™ï¼š{str(e)}")


if __name__ == "__main__":
    # _env = WorkDIR / ('dev.json' if (WorkDIR / 'dev.json').exists() else 'config.json')
    # config = json.load(_env.open('r', encoding='utf-8'))
    # while True:
    for site in config.get("websites"):
        if not site or not isinstance(site, dict):
            continue
        asyncio.run(main(site))