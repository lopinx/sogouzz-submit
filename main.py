# -*- coding: utf-8 -*-
__author__ = "https://github.com/lopinx"
# =================================================================================================
# æ·»åŠ è°ƒè¯•æ¨¡å¼å¯åŠ¨ï¼š 
# "Google Chrome": å¿…é¡»å…ˆç»“æŸå·²å­˜åœ¨çš„Chromeæµè§ˆå™¨è¿›ç¨‹åå†å¼€å¯ï¼Œ 
# Windowsï¼šchrome.exe --headless --remote-debugging-port=39222
# Linux/MacOS:  chrome --headless --remote-debugging-port=39222
# chrome --headless --remote-debugging-port=0 # éšæœºåˆ†é…ä¸€ä¸ªå¯ç”¨ç«¯å£
#  å¯åŠ¨ -> è®¿é—®"http://localhost:39222/json/version" -> è·å–ä»¥ä¸‹JSONæ•°æ®ä¸­çš„"webSocketDebuggerUrl"
"""
``` json
{
    "Browser": "Chrome/135.0.7049.85",
    "Protocol-Version": "1.3",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36",
    "V8-Version": "13.5.212.10",
    "WebKit-Version": "537.36 (@1e112499da812a1dde62101ed601dcb93024aaff)",
    "webSocketDebuggerUrl": "ws://localhost:39222/devtools/browser/6842b2f4-1219-44d9-836d-fc5fdf65a74b"
}
```
"""
# "Microsoft Edge": å¿…é¡»å…ˆç»“æŸå·²å­˜åœ¨çš„Edgeæµè§ˆå™¨è¿›ç¨‹åå†å¼€å¯
# Windowsï¼šmsedge.exe --headless --remote-debugging-port=39333
# Linux/MacOS:  msedge --headless --remote-debugging-port=39333
# msedge --headless --remote-debugging-port=0 # éšæœºåˆ†é…ä¸€ä¸ªå¯ç”¨ç«¯å£
#  å¯åŠ¨ -> è®¿é—®"http://localhost:39333/json/version" -> è·å–ä»¥ä¸‹JSONæ•°æ®ä¸­çš„"webSocketDebuggerUrl"
# https://learn.microsoft.com/zh-cn/microsoft-edge/devtools-protocol-chromium/
"""
``` json
{
    "Browser": "Chrome/135.0.7049.85",
    "Protocol-Version": "1.3",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36",
    "V8-Version": "13.5.212.10",
    "WebKit-Version": "537.36 (@1e112499da812a1dde62101ed601dcb93024aaff)",
    "webSocketDebuggerUrl": "ws://localhost:39333/devtools/browser/6842b2f4-1219-44d9-836d-fc5fdf65a74b"
}
```
"""
# =================================================================================================
# (é¡¹ç›®æ–°å»º)å®‰è£…ä¾èµ–ï¼šuv add playwright ddddocr pyjson5
# (é¡¹ç›®é‡å»º)åŒæ­¥ä¾èµ–ï¼šuv sync 
# =================================================================================================
# playwright æµè§ˆå™¨è‡ªåŠ¨åŒ–æ¡†æ¶
# å®‰è£…æµè§ˆå™¨é©±åŠ¨Chromium, Firefox, WebKitï¼Œä¸æ·»åŠ å‚æ•°åˆ™ä¸ºå…¨éƒ¨
# uv python -m playwright install chromium
# playwright codegen  #å¼€å¯å½•åˆ¶
# ä½¿ç”¨ launchï¼šå½“éœ€è¦ç›´æ¥æ§åˆ¶æµè§ˆå™¨ç”Ÿå‘½å‘¨æœŸï¼ˆå¦‚è‡ªåŠ¨åŒ–æµ‹è¯•ï¼‰æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ launchã€‚
# playwright.chromium.launch(headless=False) # ç›´æ¥å¯åŠ¨æ–°æµè§ˆå™¨å®ä¾‹
# ä½¿ç”¨ CDP è¿æ¥ï¼šå½“éœ€è¦è¿æ¥å·²æœ‰æµè§ˆå™¨å®ä¾‹ï¼ˆå¦‚å¤ç”¨ç™»å½•çŠ¶æ€ã€è¿œç¨‹è°ƒè¯•ï¼‰æ—¶ï¼Œé€šè¿‡ connect_over_cdp è¿æ¥ã€‚
# playwright.chromium.connect_over_cdp("ws://localhost:9222") # è¿æ¥å·²è¿è¡Œæµè§ˆå™¨å®ä¾‹
# browser = p.chromium.launch(channel="chrome"ï¼Œheadless=False)
# browser = p.chromium.launch(channel="msedge"ï¼Œheadless=False)
# browser = p.firefox.launch(headless=False)
# browser = p.webkit.launch(headless=False)
# =================================================================================================
# è¿è¡Œå‰æ­¥éª¤ï¼š
# 1. å®‰è£…å¹¶å¯åŠ¨ Lightpanda æœåŠ¡(CentOS)æˆ–è€…æœ¬åœ°å¼€å¯Debuggeræ¨¡å¼ï¼ˆWin+Rï¼‰ï¼š msedge.exe --headless --remote-debugging-port=39333
"""
``` bash
sudo yum install -y libappindicator-gtk3 liberation-fonts \
&& curl -L -o lightpanda https://github.com/lightpanda-io/browser/releases/download/nightly/lightpanda-x86_64-linux \
&& sudo mv lightpanda /usr/local/bin/ \
&& sudo chmod a+x /usr/local/bin/lightpanda \
&& echo 'export LIGHTPANDA_DISABLE_TELEMETRY=true' | sudo tee -a /etc/profile && sudo sh -c 'source /etc/profile' \
&& nohup lightpanda serve --host 0.0.0.0 --port 9222 > /dev/null 2>&1 & exit
```
"""
# 2. å®‰è£…ä¾èµ–ï¼šuv sync
# [CDPæ¨¡å¼ä¸éœ€è¦]3. å®‰è£…é©±åŠ¨ï¼šuv run python -m playwright install chromium
# 4. è¿è¡Œç¨‹åºï¼šuv run python main.py
# 5. æ‰“åŒ…EXEï¼šuv run pyinstaller main.spec
# =================================================================================================
import argparse
import asyncio
import logging
import random
import re
from pathlib import Path
# import threading
from typing import List
from urllib.error import HTTPError, URLError
from urllib.parse import urljoin, urlparse
from urllib.request import Request, urlopen

import ddddocr
import psutil
import pyjson5 as json
import websockets
from playwright.async_api import async_playwright
# from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed

# =================================================================================================
OCR = ddddocr.DdddOcr(show_ad=False, beta=True)
OCR.set_ranges("0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ+-x/=*") # è¯†åˆ«èŒƒå›´
# =================================================================================================
WorkDIR = Path(__file__).resolve().parent
# æ—¥å¿—è®°å½•é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - L%(lineno)d - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logging.info(f"ğŸš€ å¯åŠ¨ç¨‹åº {__author__}")
# è¯»å–é…ç½®æ–‡ä»¶dev.json,config.jsonåˆ†åˆ«ä¸ºå¼€å‘å’Œç”Ÿäº§ç¯å¢ƒ
parser = argparse.ArgumentParser(description="é…ç½®æ–‡ä»¶å‚æ•°")
parser.add_argument(
    "conf",
    type=str,
    help="æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ï¼šconfig.jsonï¼Œé»˜è®¤ä½¿ç”¨å½“å‰ç›®å½•çš„ config.jsonï¼‰",
    nargs='?',  # å…è®¸ä¸ä¼ å…¥å‚æ•°
    default= 'dev.json' if (WorkDIR / 'dev.json').exists() else 'config.json'
)
args = parser.parse_args()
if not (conf := Path(WorkDIR) / args.conf).exists():
    raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ {conf} ä¸å­˜åœ¨ï¼")
with conf.open('r', encoding='utf-8') as f:
    config = json.load(f)
logging.info(f"ğŸ¥° é…ç½®æ–‡ä»¶ {conf} åŠ è½½æˆåŠŸï¼")
# =================================================================================================

async def wslink() -> List[str]:
    """è·å–æœ¬åœ°è°ƒè¯•é“¾æ¥"""
    logging.info(f"ğŸ¥° å¼€å§‹éå†æœ¬åœ°å·²å¼€å¯è°ƒè¯•æ¨¡å¼çš„æµè§ˆå™¨ï¼")
    urls = []
    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
        if name := proc.info['name'].lower():
            # è¿›ç¨‹åç§°è¿‡æ»¤ï¼ˆChrome/Edge/Chromiumï¼‰
            if not re.match(r'^(chrome|edge|msedge|chromium)(\.exe)?$', name, re.IGNORECASE) and len(name) > 1:
                continue
        try:
            cmdline = ' '.join(proc.info['cmdline'] or []).lower()
            if '--remote-debugging-port=' not in cmdline:
                continue
            port_match = re.search(r'--remote-debugging-port=(\d+)', cmdline)
            if not port_match:
                continue
            port = int(port_match.group(1))
            if port == 0:
                for conn in proc.net_connections():
                    if conn.status == 'LISTEN' and 1024 <= (port := conn.laddr.port) <= 65535:
                        break
            try:
                req = Request(f"http://localhost:{port}/json/version", headers={'User-Agent': 'Mozilla/5.0'})
                with urlopen(req, timeout=1) as r:
                    if ws := json.loads(r.read().decode()).get('webSocketDebuggerUrl'):
                        urls.append(ws)
            except:
                continue
        except:
            continue
    return list(set(urls))

async def captcha(page: str, captcha_selector: str, input_selector: str, OCR: any) -> None:
    """å¤„ç†éªŒè¯ç çš„é€šç”¨å‡½æ•°"""
    if await page.locator(captcha_selector).is_visible():
        await page.wait_for_timeout(1000)
        captcha_obj = page.locator(captcha_selector)
        captcha_code = OCR.classification(await captcha_obj.screenshot(type='png'))
        await page.fill(input_selector, captcha_code)
        await page.wait_for_timeout(1000)
    else:
        logging.warning("âŒ éªŒè¯ç å…ƒç´ æœªæ‰¾åˆ°ï¼Œé¡µé¢å¯èƒ½æœªå®Œå…¨åŠ è½½")

async def login(page: str, site: dict, OCR: any) -> bool:
    """å¤„ç†ç™»å½•é€»è¾‘"""
    _login = False
    if await page.locator('//a[@class="logout"]').is_visible(timeout=180000):
        _login = True
        logging.info(f"âœ… ã€æ— éœ€ç™»å½•ã€‘ï¼Œå› ä¸ºå·²ç»ç™»å½•è¿‡äº†ï¼")
    else:
        await page.wait_for_timeout(1000)
        await page.click('//a[@class="login"]')
        await page.wait_for_timeout(1000)
        await page.fill('//input[@type="text" and @placeholder="å¹³å°è´¦å·"]', site['username'])
        await page.fill('//input[@type="password" and @placeholder="ç™»é™†å¯†ç "]', site['password'])
        await page.wait_for_timeout(1000)
        for attempt in range(config.get('captcha', 3)):
            try:
                await captcha(
                    page, 
                    '//*[@class="code_img"]/img',
                    '//input[@type="text" and @placeholder="éªŒè¯ç "]',
                    OCR
                )
            except Exception as e:
                logging.error(f"âŒ éªŒè¯ç å¤„ç†å¤±è´¥ï¼š{str(e)}")
                continue
            await page.wait_for_timeout(1000)
            await page.click('//a[@class="btn-login"]')
            await page.wait_for_timeout(1000)
            if await page.locator('//a[@class="logout"]').is_visible(timeout=180000):
                _login = True
                logging.info(f"âœ… ã€ç¬¬ {attempt+1} æ¬¡ã€‘ç™»å½•æˆåŠŸï¼")
                break
            else:
                await page.click('//a[@class="code_img"]')
                await page.wait_for_timeout(1000)
                continue
    return _login

async def urls(sitemap: str) -> list:
    """ä»sitemapä¸­è·å–URLåˆ—è¡¨"""
    # logging.info(f"ğŸ’ª æ­£åœ¨å¥‹åŠ›çˆ¬å–ç½‘ç«™åœ°å›¾å“¦ï¼")
    _links, urls = "", []
    for attempt in range(config.get('captcha', 3)):
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36'}
            request = Request(sitemap, headers=headers)
            with urlopen(request) as resp:
                _links = resp.read().decode('utf-8')
            break
        except URLError as e:
            logging.error(f"âŒ æ— æ³•è·å–Sitemap: {str(e)}")
            continue

    if '<?xml' in _links[:100]:
        if '<urlset' in _links or '<sitemapindex' in _links:
            if '<sitemapindex' in _links:
                mapurls = re.findall(r'<loc>\s*(.*?)\s*</loc>', _links)
                urls = []
                for su in mapurls:
                    logging.info(f"ğŸ’ª å¼€å§‹çˆ¬å–ç½‘ç«™åœ°å›¾ï¼š{su}")
                    try:
                        with urlopen(su) as resp:
                            _sublinks = resp.read().decode('utf-8')
                        urls.extend(re.findall(r'<loc>\s*(.*?)\s*</loc>', _sublinks))
                    except URLError:
                        continue
            else:
                logging.info(f"ğŸ’ª å¼€å§‹çˆ¬å–ç½‘ç«™åœ°å›¾ï¼š{sitemap}")
                urls = re.findall(r'<loc>\s*(.*?)\s*</loc>', _links)
        elif '<rss' in _links or '<feed' in _links:
            logging.info(f"ğŸ’ª å¼€å§‹çˆ¬å–ç½‘ç«™åœ°å›¾ï¼š{sitemap}")
            urls = re.findall(r'<link>\s*(https?://.*?)\s*</link>', _links)
            if not urls:
                urls.extend(re.findall(r'<link[^>]+href="(https?://[^"]+)"', _links))
                urls.extend(re.findall(r'<link[^>]+rel="alternate"[^>]+href="(https?://[^"]+)"', _links))
    else:
        if '<!DOCTYPE html' in _links[:100] or '<html' in _links[:100]:
            logging.info(f"ğŸ’ª å¼€å§‹çˆ¬å–ç½‘ç«™åœ°å›¾ï¼š{sitemap}")
            _baseUrl = urlparse(sitemap).scheme + '://' + urlparse(sitemap).netloc
            urls = re.findall(r'<a\s+href=["\'](https?://[^"\']+)["\']', _links, re.IGNORECASE)
            # è¡¥å…¨ç›¸å¯¹é“¾æ¥å¹¶ä¸”è¿‡æ»¤éæœ¬åŸŸåçš„é“¾æ¥
            urls = [urljoin(_baseUrl, url) if not urlparse(url).netloc else url for url in urls]
            urls = [url for url in urls if urlparse(url).netloc == urlparse(_baseUrl).netloc]
        else:
            logging.info(f"ğŸ’ª å¼€å§‹çˆ¬å–ç½‘ç«™åœ°å›¾ï¼š{sitemap}")
            urls = [url.strip() for url in _links.splitlines() if url.strip()]
    # å‰”é™¤é¦–é¡µé“¾æ¥
    urls = list(dict.fromkeys(url for url in urls if not url.endswith('/') and not urlparse(url).path == ''))
    return urls

async def submit(page: str, _batch: list, site: dict, OCR: any, _index: int) -> bool:
    """æäº¤URLæ‰¹æ¬¡"""
    _submit, _box = False, False
    domain = urlparse(site['sitemap']).netloc
    for attempt in range(config.get('captcha', 3)):
        try:
            await page.click('//div[contains(@class,"website_box")]')
            _box = True
            break
        except Exception as e:
            if 'Page.click: Timeout' in str(e):
                await page.evaluate("""() => {
                    const hideElements = (selectors) => {
                        selectors.forEach(selector => {
                            const el = document.querySelector(selector);
                            if (el) el.style.display = 'none';
                        });
                    };
                    hideElements(['.mask_bg', '.pop_del_area']);
                }""")
                element = page.locator("//div[contains(@class,'website_box')]")
                await element.wait_for(state="visible", timeout=10000)
                await element.evaluate("elem => elem.click()")
                _box = True
                break
            else:
                logging.error(f"âŒ ç‚¹å‡»å…ƒç´ å¤±è´¥ï¼š{str(e)}")
                await page.wait_for_timeout(1000)
                continue
    if not _box:
        logging.error(f"âŒ æ— æ³•æ‰¾åˆ°å…ƒç´ ï¼Œå¯èƒ½æ˜¯é¡µé¢åŠ è½½ä¸å®Œå…¨æˆ–å…ƒç´ ä¸å­˜åœ¨ã€‚")
        return False
    try:
        await page.wait_for_timeout(1000)
        await page.fill('//input[contains(@type,"text") and @class="search_input"]', domain)
        await page.wait_for_timeout(1000)
        await page.click(f"//li[contains(@class, 'select_item') and normalize-space()='{domain}']")
    except Exception as e:
        logging.error(f"âŒ æ²¡æœ‰æ‰¾åˆ°æ‚¨çš„åŸŸåï¼Œè¯·ç¡®è®¤æ‚¨æ‹¥æœ‰è¯¥ç«™ç‚¹æƒé™!!!")
        return False
    await page.wait_for_timeout(1000)
    await page.fill('//div[@class="form-control"]//textarea', '\n'.join(_batch))
    await page.wait_for_timeout(1000)
    for attempt in range(config.get('captcha', 3)):
        try:
            await captcha(
                page,
                '//*[@class="form-control verification"]//img',
                '//*[@class="form-control verification"]//input[@type="text"]',
                OCR
            )
        except Exception as e:
            logging.error(f"âŒ éªŒè¯ç å¤„ç†å¤±è´¥ï¼š{str(e)}")
            continue
        await page.wait_for_timeout(1000)
        await page.click('//a[@class="btn_add"]')
        await page.wait_for_timeout(1000)
        if await page.locator('//a[@class="btn_pop"]').is_visible(timeout=180000):
            await page.click('//a[@class="btn_pop"]')
            await page.wait_for_timeout(1000)
            _submit = True
            logging.info(f"âœ… ã€ç¬¬ {_index + 1} æ‰¹ - ç¬¬ {attempt+1} æ¬¡ã€‘ï¼Œæ¨é€ {len(_batch)} æ¡URL æˆåŠŸï¼")
            break
        else:
            await page.click('//*[@class="form-control verification"]//img')
            await page.wait_for_timeout(1000)
            continue
    return _submit

async def main(site: dict) -> bool:
    # è¯»å–æ—¥å¿—å¹¶è¿‡æ»¤å·²å¤„ç†çš„URL
    try:
        urls_list = await urls(site['sitemap'])
        if not urls_list:
            logging.error("âŒ æœªèƒ½ä»æ‚¨æä¾›çš„Sitemapåœ°å€è·å–åˆ°URL")
            return False
        
        _cps = []
        try:
            with open(WorkDIR / f'{urlparse(site['sitemap']).netloc}.log', 'r') as f:
                lines = f.read().splitlines()
                _cps = [int(line.strip()) for line in lines if line.strip().isdigit()]
        except FileNotFoundError:
            logging.warning("âš ï¸  æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†é‡æ–°å¤„ç†æ‰€æœ‰URL")

        urls_list = [url for idx, url in enumerate(urls_list) if idx not in _cps]
        if not urls_list:
            logging.error("âŒ æ‰€æœ‰URLå‡å·²å¤„ç†å®Œæ¯•ä¸”æ²¡æœ‰æ–°çš„URLåœ°å€")
            return False
    except URLError as e:
        logging.error(f"âŒ ç½‘ç»œé”™è¯¯ï¼Œæ— æ³•è·å–: {str(e)}")
        return False
    try:
        async with async_playwright() as playwright:
            # æµè§ˆå™¨çš„å‚æ•°
            _cargs = {
                "viewport": {"width": 1440, "height": 900},     # è®¾ç½®æµè§ˆå™¨çª—å£å¤§å°
                "screen": {"width": 1920, "height": 1080},      # è®¾ç½®å±å¹•åˆ†è¾¨ç‡
                "ignore_https_errors": True,                    # å¿½ç•¥httpsé”™è¯¯
                "java_script_enabled": True,                    # å¯ç”¨JavaScript
                "locale": "zh-CN",                              # è®¾ç½®è¯­è¨€ç¯å¢ƒ
                "timezone_id": "Asia/Shanghai",                 # è®¾ç½®æ—¶åŒº
                # "geolocation": {"longitude": 116.40387, "latitude": 39.91435, "accuracy": 100},   # è®¾ç½®åœ°ç†ä½ç½®
                "permissions": ["notifications"],               # è®¾ç½®æƒé™: é€šçŸ¥
                "bypass_csp": True,                             # ç»•è¿‡ CSPï¼ˆContent Security Policyï¼‰
                "accept_downloads": True,                       # å…è®¸ä¸‹è½½
                "user_agent": 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36'
            }
            
            # ç­›é€‰å¯ç”¨WSé“¾æ¥
            ws_link = []
            if cdp := config.get("cdpserver"):
                for url in cdp:
                    try:
                        async with websockets.connect(url, open_timeout=5) as ws:
                            await ws.ping()
                            await ws.close()
                            ws_link.append(url)
                    except: 
                        continue
            else:
                ws_link.extend(await wslink())
            
            if ws_link:
                _ws = random.choice(ws_link)
            else:
                logging.error("âŒ æœªæ‰¾åˆ°å¯ç”¨CDPæœåŠ¡å™¨ï¼Œå°†ä½¿ç”¨æœ¬åœ°æµè§ˆå™¨")
                return False

            # è¿æ¥åˆ°æµè§ˆå™¨        
            try:
                browser = await playwright.chromium.connect_over_cdp(
                    _ws, 
                    timeout=180000,
                    slow_mo=500
                )
            except Exception as e:
                logging.error(f"âŒ æ— æ³•è¿æ¥åˆ°CDPæœåŠ¡: {str(e)}")
                return False
            
            try:
                context = browser.contexts[0]
                page = context.pages[0] if context.pages else await context.new_page()
            except (IndexError, TypeError, Exception) as e:
                context = await browser.new_context(**_cargs)
                page = await context.new_page()
            
            # æµ‹è¯•ååçˆ¬è™«
            await page.goto('https://res.cdn.issem.cn', wait_until='networkidle', timeout=180000)
            
            # ç™»å½•ç«™é•¿å¹³å°
            await page.goto(config.get("backend"), wait_until='networkidle', timeout=180000)
            await page.wait_for_timeout(1000)
            if not await login(page, site, OCR):
                return False

            await page.goto(f"{config.get('backend')}index.php/sitelink/index", wait_until='networkidle', timeout=180000)
            
            # åˆ†æ‰¹å¤„ç†é“¾æ¥
            # _sizeä¸ºæ¯æ‰¹æ¬¡æ•°é‡ï¼Œbatchä¸ºå·²å®Œæˆæ‰¹æ¬¡ï¼Œbatchesä¸ºæ€»æ‰¹æ¬¡
            # _bacthä¸ºå½“å‰æ‰¹æ¬¡ï¼Œ_bTagä¸ºå½“å‰æ‰¹æ¬¡æ˜¯å¦æˆåŠŸ
            _size = 20
            batch, batches = set(), (len(urls_list) + _size - 1) // _size
            while len(batch) < batches:
                for _index, _start in enumerate(range(0, len(urls_list), _size)):
                    if _index in batch:
                        continue
                    _batch, _bTag = urls_list[_start:_start + _size], False     
                    for _ in range(config.get('captcha', 3)):
                        try:
                            if await submit(page, _batch, site, OCR, _index):
                                _bTag = True
                                batch.add(_index)
                                with open(WorkDIR / f'{urlparse(site['sitemap']).netloc}.log', 'a+') as f:
                                    for url in _batch:
                                        f.write(f"{url}\n")
                                break
                            await page.wait_for_timeout(1000)
                        except Exception as e:
                            await page.wait_for_timeout(1000)
                            continue 
                    if not _bTag:
                        logging.warning(f"âš ï¸  ç¬¬ {_index + 1} æ‰¹æ‰€æœ‰å°è¯•å‡å¤±è´¥ï¼Œå°†åœ¨ä¸‹æ¬¡å¾ªç¯é‡è¯•")
                if len(batch) < batches:
                    logging.warning(f"â³ æ‰¹æ¬¡å¤„ç†æœªå®Œæˆï¼š{len(batch)}/{batches}ï¼Œå‡†å¤‡å¼€å§‹æ–°ä¸€è½®é‡è¯•...")
            return True
    except Exception as e:
        logging.error(f"âŒ å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
        return False
    finally:
        try:
            if 'page' in locals() and page:
                await page.close()
            if 'context' in locals() and context:
                await context.close()
            if 'browser' in locals() and browser and browser.is_connected():
                await browser.close()
        except Exception as e:
            logging.error(f"âš ï¸  æ¸…ç†èµ„æºæ—¶å‡ºé”™ï¼š{str(e)}")


if __name__ == "__main__":
    # _env = WorkDIR / ('dev.json' if (WorkDIR / 'dev.json').exists() else 'config.json')
    # config = json.load(_env.open('r', encoding='utf-8'))
    # while True:
    for site in config.get("websites"):
        if not site or not isinstance(site, dict):
            continue
        asyncio.run(main(site))